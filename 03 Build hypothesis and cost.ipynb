{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $H(x) = Wx + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [1, 2, 3, 4, 5]\n",
    "y_data = [1, 2, 3, 4, 5]\n",
    "\n",
    "W = tf.Variable(2.9)\n",
    "b = tf.Variable(0.5)\n",
    "\n",
    "hypothesis = (W * x_data) + b\n",
    "\n",
    "# x와 y가 동일할 때, W = 1, b =0이 나와야 성립이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"http://latex.codecogs.com/svg.latex?cost(W, b) = \\frac{1}{m}\\sum (H(x^{i}) - y^i)^2\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y^ - y의 제곱의 전체 합 평균\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.reduce_mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = [1., 2., 3., 4.]\n",
    "\n",
    "sess = tf.Session()\n",
    "tf.reduce_mean(v) # 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.square(3) # 3 ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent\n",
    "- $Minimize$ $cost(W, b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    hypothesis = (W * x_data) + b\n",
    "    cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "    \n",
    "W_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "W.assign_sub(learning_rate * W_grad)\n",
    "b.assign_sub(learning_rate * b_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 | 2.4520 | 0.3760 | 45.660004\n",
      "10 | 1.1036 | 0.0034 | 0.206336\n",
      "20 | 1.0128 | -0.0209 | 0.001026\n",
      "30 | 1.0065 | -0.0218 | 0.000093\n",
      "40 | 1.0059 | -0.0212 | 0.000083\n",
      "50 | 1.0057 | -0.0205 | 0.000077\n",
      "60 | 1.0055 | -0.0198 | 0.000072\n",
      "70 | 1.0053 | -0.0192 | 0.000067\n",
      "80 | 1.0051 | -0.0185 | 0.000063\n",
      "90 | 1.0050 | -0.0179 | 0.000059\n",
      "100 | 1.0048 | -0.0173 | 0.000055\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "x_data = [1, 2, 3, 4, 5]\n",
    "y_data = [1, 2, 3, 4, 5]\n",
    "\n",
    "W = tf.Variable(2.9)\n",
    "b = tf.Variable(0.5)\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "for i in range(100 + 1):\n",
    "    # Gradient descent\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = (W * x_data) + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "        \n",
    "    W_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "    W.assign_sub(learning_rate * W_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "    if i % 10 == 0:\n",
    "        print('{} | {:.4f} | {:.4f} | {:.6f}'.format(i, W.numpy(), b.numpy(), cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
